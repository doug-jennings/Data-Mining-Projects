{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 484 :: Data Mining :: George Mason University :: Spring 2023\n",
    "\n",
    "\n",
    "# Homework 3: Clustering&Association Rule Mining\n",
    "\n",
    "- **100 points [9% of your final grade]**\n",
    "- **Due Sunday, April 16 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* (1) implement your K-means model; and (2) implement the association rule mining process with the Apriori algorithm.\n",
    "\n",
    "- *Submission instructions:* for this homework, you only need to submit to Blackboard. Please name your submission **FirstName_Lastname_hw3.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw3.ipynb**. Your notebook should be fully executed so that we can see all outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Clustering (50 points)\n",
    "\n",
    "In this part, you will implement your own K-means algorithm to conduct clustering on handwritten digit images. In this homework, we will still use the handwritten digit image dataset we have already used in previous homework. However, since clustering is unsupervised learning, which means we do not use the label information anymore. So, here, we will only use the testing data stored in the \"test.txt\" file.\n",
    "\n",
    "First, let's load the data by excuting the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of testing feature matrix: shape (10000, 784)\n",
      "array of testing label matrix: shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "test_features = test[:, 1:]\n",
    "test_labels = test[:, 0]\n",
    "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
    "print('array of testing label matrix: shape ' + str(np.shape(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for you to implement your own K-means algorithm. First, please write your code to build your K-means model using the image data with **K = 10**, and **Euclidean distance**.\n",
    "\n",
    "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn**\n",
    "\n",
    "**Note: you need to decide when to stop the model training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "\n",
    "# Load dataset\n",
    "dataset = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "features = dataset[:, 1:]\n",
    "labels = dataset[:, 0]\n",
    "\n",
    "class KMeans:\n",
    "    \n",
    "    def __init__(self, num_clusters=10, max_iterations=100, tolerance=0.0001):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "    def fit(self, data):\n",
    "        # Initialize centroids randomly\n",
    "        self.centroids = data[np.random.choice(data.shape[0], self.num_clusters, replace=False)]        \n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Assign each instance to the closest centroid\n",
    "            distances = np.sqrt(((data - self.centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "            assigned_clusters = np.argmin(distances, axis=0)\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids = np.zeros((self.num_clusters, data.shape[1]))\n",
    "            for cluster_index in range(self.num_clusters):\n",
    "                new_centroids[cluster_index] = np.mean(data[assigned_clusters == cluster_index], axis=0)\n",
    "            \n",
    "            # Check if convergence criterion is met\n",
    "            if np.all(np.sqrt(((new_centroids - self.centroids)**2).sum(axis=1)) < self.tolerance):\n",
    "                break\n",
    "                \n",
    "            self.centroids = new_centroids\n",
    "\n",
    "    def predict(self, data):\n",
    "        distances = np.sqrt(((data - self.centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "        return np.argmin(distances, axis=0)\n",
    "    \n",
    "\n",
    "# Create and fit K-means model\n",
    "kmeans_model = KMeans()\n",
    "kmeans_model.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to calculate the square root of Sum of Squared Error (SSE) of each cluster generated by your K-means algorithm. Then, print out the averaged SSE of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged SSE of K-means algorithm: 2538658.051873758\n"
     ]
    }
   ],
   "source": [
    "# Calculate SSE for each cluster\n",
    "assigned_clusters = kmeans_model.predict(features)\n",
    "cluster_centroids = kmeans_model.centroids[assigned_clusters]\n",
    "sse_per_cluster = np.sum((features - cluster_centroids)**2, axis=1)\n",
    "sse = np.sum(sse_per_cluster)\n",
    "\n",
    "# Calculate average SSE\n",
    "average_sse = sse_per_cluster.mean()\n",
    "\n",
    "# Print averaged SSE\n",
    "print('Averaged SSE of K-means algorithm: ' + str(average_sse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, please have a look on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure, and use this function to print out the homogeneity, completeness, and v-measure of your K-means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.5080176516215134\n",
      "Completeness: 0.5140663061668767\n",
      "V-measure: 0.5110240810387741\n"
     ]
    }
   ],
   "source": [
    "# Calculate homogeneity, completeness, and v-measure\n",
    "homogeneity_score, completeness_score, v_measure_score = homogeneity_completeness_v_measure(labels, kmeans_model.predict(features))\n",
    "\n",
    "# Print the results\n",
    "print('Homogeneity: ' + str(homogeneity_score))\n",
    "print('Completeness: ' + str(completeness_score))\n",
    "print('V-measure: ' + str(v_measure_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Association Rule Mining (50 points)\n",
    "\n",
    "In this part, you are going to examine movies using our understanding of association rules. For this part, you need to implement the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of user-movie matrix: shape (11743, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
    "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, there are two columns: the first column is the integer ids of users, and the second column is the integer ids of movies. Each row denotes that the user of given user id rated the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been rated by a single user as a transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user rating behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. \n",
    "\n",
    "**Note: Do not copy-paste any existing code.**\n",
    "\n",
    "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
    "\n",
    "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will highlight the method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates of length 1 count: 408\n",
      "After Pruning count: 21\n",
      "Candidates of length 2 count: 210\n",
      "After Pruning count: 36\n",
      "Candidates of length 3 count: 55\n",
      "After Pruning count: 12\n",
      "Candidates of length 4 count: 1\n",
      "After Pruning count: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Load user-movie data from the file\n",
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
    "\n",
    "# Load movie names from the movies.csv file\n",
    "movies_df = pd.read_csv(\"movies.csv\")\n",
    "movie_names = dict(zip(movies_df['movieId'], movies_df['movie_name']))\n",
    "\n",
    "# Convert the data into a list of transactions\n",
    "user_transactions = {}\n",
    "# Iterate through each row in the user_movie_data\n",
    "for row in user_movie_data:\n",
    "    user_id = int(row[0])\n",
    "    movie_id = int(row[1])\n",
    "    # Create a set for each user to store the movies they have watched\n",
    "    if user_id not in user_transactions:\n",
    "        user_transactions[user_id] = set()\n",
    "    # Add the movie to the user's set of watched movies\n",
    "    user_transactions[user_id].add(movie_id)\n",
    "\n",
    "# Convert the user_transactions dictionary to a list of transactions\n",
    "transactions = list(user_transactions.values())\n",
    "\n",
    "# Function to generate frequent single itemsets (1-itemsets)\n",
    "def generate_frequent_single_itemsets(transactions, min_support):\n",
    "    # Count the occurrences of each item in the transactions\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if item not in item_counts:\n",
    "                item_counts[item] = 0\n",
    "            item_counts[item] += 1\n",
    "\n",
    "    # Identify the frequent 1-itemsets based on the min_support threshold\n",
    "    frequent_single_itemsets = set()\n",
    "    num_transactions = len(transactions)\n",
    "    for item, count in item_counts.items():\n",
    "        support = count / num_transactions\n",
    "        if support >= min_support:\n",
    "            frequent_single_itemsets.add(frozenset([item]))\n",
    "\n",
    "    # Print the number of candidates and frequent 1-itemsets\n",
    "    print(\"Candidates of length 1 count: \" + str(len(item_counts)))\n",
    "    print(\"After Pruning count: \" + str(len(frequent_single_itemsets)))\n",
    "\n",
    "    return frequent_single_itemsets\n",
    "\n",
    "# Function to generate frequent k-itemsets (k > 1)\n",
    "def generate_frequent_itemsets(transactions, k, prev_frequent_itemsets, min_support):\n",
    "    # Generate candidate k-itemsets by joining (k-1)-itemsets\n",
    "    candidate_itemsets = set()\n",
    "    prev_frequent_itemsets_list = list(prev_frequent_itemsets)\n",
    "    for i in range(len(prev_frequent_itemsets_list)):\n",
    "        for j in range(i + 1, len(prev_frequent_itemsets_list)):\n",
    "            itemset1 = prev_frequent_itemsets_list[i]\n",
    "            itemset2 = prev_frequent_itemsets_list[j]\n",
    "            union = itemset1.union(itemset2)\n",
    "            # Check if the union has k items\n",
    "            if len(union) == k:\n",
    "                itemset1_sorted = sorted(itemset1)\n",
    "                itemset2_sorted = sorted(itemset2)\n",
    "                # Check if the (k-1)-itemsets share the same (k-2) items\n",
    "                if itemset1_sorted[:-1] == itemset2_sorted[:-1]:\n",
    "                    # Check if all (k-1)-item subsets of the union are frequent\n",
    "                    all_subsets_frequent = True\n",
    "                    for subset in itertools.combinations(union, k - 1):\n",
    "                        if frozenset(subset) not in prev_frequent_itemsets:\n",
    "                            all_subsets_frequent = False\n",
    "                            break\n",
    "                    # If all (k-1)-item subsets are frequent, add the union to the candidates\n",
    "                    if all_subsets_frequent:\n",
    "                        candidate_itemsets.add(union)\n",
    "\n",
    "    # Count the occurrences of candidate k-itemsets in the transactions\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for candidate_itemset in candidate_itemsets:\n",
    "            # Check if the candidate k-itemset is a subset of the transaction\n",
    "            if candidate_itemset.issubset(transaction):\n",
    "                if candidate_itemset not in item_counts:\n",
    "                    item_counts[candidate_itemset] = 0\n",
    "                item_counts[candidate_itemset] += 1\n",
    "\n",
    "    # Identify the frequent k-itemsets based on the min_support threshold\n",
    "    frequent_itemsets = set()\n",
    "    num_transactions = len(transactions)\n",
    "    for itemset, count in item_counts.items():\n",
    "        support = count / num_transactions\n",
    "        if support >= min_support:\n",
    "            frequent_itemsets.add(itemset)\n",
    "\n",
    "    # Print the number of candidates and frequent k-itemsets\n",
    "    print(\"Candidates of length \" + str(k) + \" count: \" + str(len(candidate_itemsets)))\n",
    "    print(\"After Pruning count: \" + str(len(frequent_itemsets)))\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "# Function to generate association rules from the frequent itemsets\n",
    "def generate_association_rules(transactions, frequent_itemsets, min_confidence):\n",
    "    association_rules = []\n",
    "    num_transactions = len(transactions)\n",
    "\n",
    "    # Calculate the support of all frequent itemsets\n",
    "    itemset_support = {}\n",
    "    for transaction in transactions:\n",
    "        for itemset in frequent_itemsets:\n",
    "            if itemset.issubset(transaction):\n",
    "                if itemset not in itemset_support:\n",
    "                    itemset_support[itemset] = 0\n",
    "                itemset_support[itemset] += 1\n",
    "\n",
    "    # Calculate the support of all antecedents\n",
    "    antecedent_support = {}\n",
    "    for transaction in transactions:\n",
    "        for itemset in frequent_itemsets:\n",
    "            if len(itemset) > 1:\n",
    "                # Create all possible antecedents of length i from the itemset\n",
    "                for i in range(1, len(itemset)):\n",
    "                    for antecedent in itertools.combinations(itemset, i):\n",
    "                        antecedent = frozenset(antecedent)\n",
    "                        if antecedent.issubset(transaction):\n",
    "                            if antecedent not in antecedent_support:\n",
    "                                antecedent_support[antecedent] = 0\n",
    "                            antecedent_support[antecedent] += 1\n",
    "\n",
    "    # Generate association rules from frequent itemsets\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in itertools.combinations(itemset, i):\n",
    "                    antecedent = frozenset(antecedent)\n",
    "                    consequent = itemset.difference(antecedent)\n",
    "                    antecedent_count = antecedent_support[antecedent]\n",
    "                    itemset_count = itemset_support[itemset]\n",
    "                    confidence = itemset_count / antecedent_count\n",
    "                    # Check if the confidence is above the min_confidence threshold\n",
    "                    if confidence >= min_confidence:\n",
    "                        # Convert movie IDs to movie names\n",
    "                        antecedent_names = [movie_names[movie_id] for movie_id in antecedent]\n",
    "                        consequent_names = [movie_names[movie_id] for movie_id in consequent]\n",
    "                        # Create a rule string and add it to the association_rules list\n",
    "                        rule = \", \".join(antecedent_names) + \" -> \" + \", \".join(consequent_names)\n",
    "                        association_rules.append(rule)\n",
    "\n",
    "    return association_rules\n",
    "\n",
    "# Parameters\n",
    "min_support = 0.2\n",
    "min_confidence = 0.8\n",
    "\n",
    "# Generate frequent 1-itemsets\n",
    "frequent_1_itemsets = generate_frequent_single_itemsets(transactions, min_support)\n",
    "\n",
    "# Generate frequent itemsets of length k and association rules\n",
    "frequent_itemsets = frequent_1_itemsets\n",
    "k = 2\n",
    "rules = []\n",
    "while len(frequent_itemsets) > 0:\n",
    "    prev_frequent_itemsets = frequent_itemsets\n",
    "    # Generate frequent k-itemsets using the previous frequent itemsets\n",
    "    frequent_itemsets = generate_frequent_itemsets(transactions, k, prev_frequent_itemsets, min_support)\n",
    "    # Generate association rules from the current frequent itemsets\n",
    "    association_rules = generate_association_rules(transactions, frequent_itemsets, min_confidence)\n",
    "    # Add the generated association rules to the list of rules\n",
    "    for rule in association_rules:\n",
    "        rules.append(rule)\n",
    "    # Increment k for the next iteration\n",
    "    k += 1\n",
    "\n",
    "# At this point, the 'rules' list contains the association rules that satisfy the given min_support and min_confidence thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print your final association rules in the following format:\n",
    "\n",
    "**movie_name_1, movie_name_2, ... --> movie_name_k**\n",
    "\n",
    "where the movie names can be fetched by joining the movieId with the file 'movies.csv'. For example, one rule that you should find is:\n",
    "\n",
    "**Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)**\n",
    "\n",
    "**Hint: You may need to use the Pandas library to load and process the movies.csv file, such as use pandas.read_csv() to load the data. https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html is a good place to learn the basics about Pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaws (1975) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Jurassic Park (1993), Princess Bride, The (1987) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Jurassic Park (1993), Back to the Future (1985) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Saving Private Ryan (1998), Back to the Future (1985) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Back to the Future (1985), Schindler's List (1993) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Saving Private Ryan (1998), Princess Bride, The (1987) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Jurassic Park (1993), Saving Private Ryan (1998) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Godfather, The (1972), Godfather: Part II, The (1974) -> Star Wars: Episode IV - A New Hope (1977)\n",
      "Star Wars: Episode IV - A New Hope (1977), Godfather: Part II, The (1974) -> Godfather, The (1972)\n"
     ]
    }
   ],
   "source": [
    "# print the association rules\n",
    "for rule in rules:\n",
    "    print(rule)\n",
    "    \n",
    "# Based on a piazza comment from the instructor these appear to be correct, but there are 5 rules missing."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
